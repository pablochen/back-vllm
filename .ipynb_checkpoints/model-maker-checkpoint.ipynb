{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9102cd9a-d36c-4460-b7e5-e518b5e302f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f99134-df3d-4f6d-9b63-985140466a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f92b93a7d284085990cf83376f775e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: model.embed_tokens.weight, Type: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3.5-mini-instruct\", \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Type: {param.dtype}\")\n",
    "    break  # 첫 번째 파라미터만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3def11-3b5d-48ac-bcbe-03035ad1b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_model(model, tokenizer, input_text, token_len):\n",
    "    start_time = time.time()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"너는 도움되는 AI 비서야. 차근 차근 생각해서 답을 알려줘\"},\n",
    "        {\"role\": \"user\", \"content\": input_text},\n",
    "    ]\n",
    "    \n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": token_len,\n",
    "        \"return_full_text\": False,\n",
    "        \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "    \n",
    "    output = pipe(messages, **generation_args)\n",
    "    end_time = time.time()  # 종료 시간 기록\n",
    "    execution_time = end_time - start_time  # 실행 시간 계산\n",
    "    print(f\"Execution Time: {execution_time:.6f} seconds\")\n",
    "\n",
    "    return output[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875067d0-2f60-46ef-9eaf-e4ec6c9804cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 40.531145 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 한국에는 다양한 종류의 새들이 서식하고 있으며, 이들은 다양한 환경과 식물의 번성에 기여하고 있습니다. 다음은 한국의 주요 새들의 세부 정보와 그들의 특성에 대한 설명입니다:\\n\\n1. 노랑새 (Cettia tenebrosa): 이 새는 녹색과 갈색의 밝은 색상으로 유명합니다. 낮에는 잎으로 덮인 숲에서 생활하며, 먹이를 찾기 위해 잎을 뒤집습니다. 이 새는 번식기에 매우 소음이 많으며, 낮에는 매우 빠르게 날아다니며 번식을 위해 번식지로 이동합니다.\\n\\n2. 흰머리새 (Zosterops japonicus): 이 새는 흰머리새의 한 종류로, 날개와 머리에 흰색 반점이 있습니다. 이 새는 잎으로 덮인 숲과 농지에서 번식하며, 먹이를 찾기 위해 잎을 뒤집습니다. 흰머리새는 매우 빠르게 날아다니며 번식을 위해 번식지로 이동합니다.\\n\\n3. 노랑딱등새 (Sylvia cantillans): 이 새는 노란색과 갈색의 밝은 색상으로 유명합니다. 이 새는 잎으로 덮인 숲과 농지에서 번식하며, 먹이를 찾기 위해 잎을 뒤집습니다. 노랑딱등새는 매우 빠르게 날아다니며 번식을 위해 번식지로 이동합니다.\\n\\n4. 노랑말벌새 (Sylvia longicauda): 이 새는 노란색과 갈색의 밝은 색상으로 유명합니다. 이 새는 잎으로 덮인 숲과 농지에서 번식하며, 먹이를 찾기 위해 잎을 뒤집습니다. 노랑말벌새는 매우 빠르게 날아다니며 번식을 위해 번식지로 이동합니다.\\n\\n5. 흰머리����'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '한국에 어떤 새들이 존재하는지 자세히 설명해봐'\n",
    "get_answer_from_model(model, tokenizer, question, 1024) # Execution Time: 40.531145 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3242c9-42cc-4053-919c-0cae653a5f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028939c-8ad6-48d9-a729-4fd034150640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01152a15-5e2b-48e3-bf06-7f2500efafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-22 08:44:56 arg_utils.py:776] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.\n",
      "INFO 08-22 08:44:56 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/Phi-3.5-mini-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-mini-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/Phi-3.5-mini-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 08-22 08:44:57 model_runner.py:720] Starting to load model microsoft/Phi-3.5-mini-instruct...\n",
      "INFO 08-22 08:44:57 weight_utils.py:225] Using model weights format ['*.safetensors']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import time\n",
    "\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"  # 사용하고자 하는 모델 이름\n",
    "tokenizer_name = \"microsoft/Phi-3.5-mini-instruct\"  # 모델에 맞는 토크나이저 이름\n",
    "\n",
    "model = LLM(model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445aeab-2fbb-41c9-9079-412a410263fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_vllm(model, tokenizer, input_text, token_len):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=token_len,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    output = model.generate(inputs, sampling_params=sampling_params)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Execution Time: {execution_time:.6f} seconds\")\n",
    "    \n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700589e-7a72-47a0-8a61-eb079a451a83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_text = \"한국에는 어떤 새들이 사는지 알려줘\"\n",
    "token_len = 1024\n",
    "response = get_answer(model, tokenizer, input_text, token_len)\n",
    "print(f\"Generated Text: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd0dd3-9422-47d2-b0e4-8d389f47b420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9646d1-f489-4d18-b9b0-db7433b75c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be151682-ba86-4a4c-b117-04aabf7a1355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27afa54-1370-4046-bb1c-3ee8589e3dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33049e-6f79-4398-9e62-f6b4a2153e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8eab6-3c60-489f-8c6c-d2e6df733be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e3dca-4944-40d5-9440-7d854d89c05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a03b2-428f-49da-9c8f-388d164186b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
